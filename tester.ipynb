{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ab79093-8305-4300-b131-a626524c7daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivatives =  -91.0 -6388.2 -1765.3999999999999\n",
      "sum_err =  [-455.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  and sum_sqrerr =  [44173.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "After Iteration  0 \n",
      "--------------------\n",
      "w0 =  0.0182  ; w1 =  2.27764  ; w2 =  1.35308\n",
      "MSE =  4417.3\n",
      "predicting for x1 = 67 and x2 = 24 gives  185.094\n",
      "\n",
      "\n",
      "derivatives =  5.558279999999996 390.1912559999998 107.83063199999992\n",
      "sum_err =  [-455.0, 27.79139999999998, 0.0, 0.0, 0.0, 0.0, 0.0]  and sum_sqrerr =  [44173.0, 1644.759367595201, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "After Iteration  1 \n",
      "--------------------\n",
      "w0 =  0.017088344000000002  ; w1 =  2.1996017488  ; w2 =  1.3315138736\n",
      "MSE =  164.4759367595201\n",
      "predicting for x1 = 67 and x2 = 24 gives  179.34673848\n",
      "\n",
      "\n",
      "derivatives =  -0.33949974239998826 -23.832881916479177 -6.586295002559772\n",
      "sum_err =  [-455.0, 27.79139999999998, -1.6974987119999412, 0.0, 0.0, 0.0, 0.0]  and sum_sqrerr =  [44173.0, 1644.759367595201, 1553.4112411522146, 0.0, 0.0, 0.0, 0.0]\n",
      "After Iteration  2 \n",
      "--------------------\n",
      "w0 =  0.01715624394848  ; w1 =  2.204368325183296  ; w2 =  1.332831132600512\n",
      "MSE =  155.34112411522148\n",
      "predicting for x1 = 67 and x2 = 24 gives  179.6977812136416\n",
      "\n",
      "\n",
      "derivatives =  0.020736644265798534 1.4557124274590572 0.40229089875649154\n",
      "sum_err =  [-455.0, 27.79139999999998, -1.6974987119999412, 0.10368322132899266, 0.0, 0.0, 0.0]  and sum_sqrerr =  [44173.0, 1644.759367595201, 1553.4112411522146, 1548.9588543566686, 0.0, 0.0, 0.0]\n",
      "After Iteration  3 \n",
      "--------------------\n",
      "w0 =  0.01715209661962684  ; w1 =  2.2040771826978043  ; w2 =  1.3327506744207607\n",
      "MSE =  154.89588543566686\n",
      "predicting for x1 = 67 and x2 = 24 gives  179.67633952347077\n",
      "\n",
      "\n",
      "derivatives =  -0.0012665942317426017 -0.08891491506833064 -0.02457192809580647\n",
      "sum_err =  [-455.0, 27.79139999999998, -1.6974987119999412, 0.10368322132899266, -0.006332971158713008, 0.0, 0.0]  and sum_sqrerr =  [44173.0, 1644.759367595201, 1553.4112411522146, 1548.9588543566686, 1549.1933793534308, 0.0, 0.0]\n",
      "After Iteration  4 \n",
      "--------------------\n",
      "w0 =  0.01715234993847319  ; w1 =  2.204094965680818  ; w2 =  1.3327555888063798\n",
      "MSE =  154.9193379353431\n",
      "predicting for x1 = 67 and x2 = 24 gives  179.6776491819064\n",
      "\n",
      "\n",
      "derivatives =  7.736357566159314e-05 0.005430923011443839 0.0015008533678349068\n",
      "sum_err =  [-455.0, 27.79139999999998, -1.6974987119999412, 0.10368322132899266, -0.006332971158713008, 0.00038681787830796566, 0.0]  and sum_sqrerr =  [44173.0, 1644.759367595201, 1553.4112411522146, 1548.9588543566686, 1549.1933793534308, 1549.1789149360231, 0.0]\n",
      "After Iteration  5 \n",
      "--------------------\n",
      "w0 =  0.017152334465758055  ; w1 =  2.2040938794962157  ; w2 =  1.3327552886357061\n",
      "MSE =  154.91789149360233\n",
      "predicting for x1 = 67 and x2 = 24 gives  179.67756918796914\n",
      "\n",
      "\n",
      "derivatives =  -4.7253672050828754e-06 -0.00033172077779681787 -9.167212377860778e-05\n",
      "sum_err =  [-455.0, 27.79139999999998, -1.6974987119999412, 0.10368322132899266, -0.006332971158713008, 0.00038681787830796566, -2.3626836025414377e-05]  and sum_sqrerr =  [44173.0, 1644.759367595201, 1553.4112411522146, 1548.9588543566686, 1549.1933793534308, 1549.1789149360231, 1549.1797979017088]\n",
      "After Iteration  6 \n",
      "--------------------\n",
      "w0 =  0.017152335410831495  ; w1 =  2.204093945840371  ; w2 =  1.3327553069701308\n",
      "MSE =  154.91797979017088\n",
      "predicting for x1 = 67 and x2 = 24 gives  179.67757407399884\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x1 = [60.,67.,71.,75.,78.]\n",
    "x2 = [22.,24.,15.,20.,16.]\n",
    "y = [140.,159.,192.,200.,212.]\n",
    "h = [0.,0.,0.,0.,0.]\n",
    "err = [0.,0.,0.,0.,0.]\n",
    "sqrerr = [0.,0.,0.,0.,0.]\n",
    "derivative_mse = [0.,0.,0.] #for w0, w1, w2 for each iteration\n",
    "grad_descent = [0.,0.,0.] #for w0, w1, w2 for each iteration\n",
    "mse = [0.,0.,0.,0.,0.,0.,0.] #for initial and 2 iterations\n",
    "sum_err = [0.,0.,0.,0.,0.,0.,0.] #for initial and 2 iterations\n",
    "sum_sqrerr = [0.,0.,0.,0.,0.,0.,0.] #for initial and 2 iterations\n",
    "alpha = 0.0002\n",
    "x1_mean = x2_mean = 0.\n",
    "\n",
    "w0 = 0.\n",
    "w1 = w2 = 1.\n",
    "\n",
    "for m in range(0,5):\n",
    "    x1_mean += x1[m]\n",
    "    x2_mean += x2[m]\n",
    "x1_mean = x1_mean / 5\n",
    "x2_mean = x2_mean / 5\n",
    "\n",
    "for i in range(0,7):\n",
    "    for j in range(0,5):\n",
    "        h[j] = w0 + w1*x1[j] + w2*x2[j]\n",
    "        err[j] = h[j] - y[j]\n",
    "        sqrerr[j] = err[j]**2\n",
    "        sum_err[i] += err[j]\n",
    "        sum_sqrerr[i] += sqrerr[j]\n",
    "    mse[i] = 1/(2*5) * sum_sqrerr[i]\n",
    "    derivative_mse[0] = 2/(2*5) * sum_err[i]\n",
    "    derivative_mse[1] = 2/(2*5) * sum_err[i] * x1_mean\n",
    "    derivative_mse[2] = 2/(2*5) * sum_err[i] * x2_mean\n",
    "    print('derivatives = ', derivative_mse[0], derivative_mse[1], derivative_mse[2])\n",
    "    print('sum_err = ', sum_err, ' and sum_sqrerr = ', sum_sqrerr)\n",
    "    w0 -= alpha * derivative_mse[0]\n",
    "    w1 -= alpha * derivative_mse[1]\n",
    "    w2 -= alpha * derivative_mse[2]\n",
    "    print('After Iteration ', i, '\\n--------------------')\n",
    "    print('w0 = ', w0, ' ; w1 = ', w1, ' ; w2 = ', w2)\n",
    "    print('MSE = ', mse[i])\n",
    "    print('predicting for x1 = 67 and x2 = 24 gives ', w0 + w1*x1[1] + w2*x2[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "addb8bc7-8ab3-4a63-bcf7-faacda87bdba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcvxopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m solvers\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcvxopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolvers\u001b[39;00m                  \u001b[38;5;66;03m# cvxopt for solving the dual optimization problem\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cvxopt'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing models - DO NOT CHANGE\n",
    "import sys\n",
    "import numpy as np\n",
    "from cvxopt import solvers\n",
    "import cvxopt.solvers                  # cvxopt for solving the dual optimization problem\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('./pulsar_star_dataset.csv')\n",
    "df.head()                                                           # reading the dataset\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']                                                                                     # splitting the dataset into features and labels\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()                                                                                    # converting the dataset into numpy array for ease of use\n",
    "y[y == 0] = -1                                                                                      # converting the labels to -1 and 1, as per the SVM problem formulation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)           # splitting the dataset into train and test set\n",
    "mean_train = X_train.mean()                                                                         # standardizing the dataset\n",
    "std_train = X_train.std()\n",
    "\n",
    "#########################################    code to be filled part a(i)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#new_ypred = 0\n",
    "#new_X = input('Enter new data point: ')\n",
    "#for c in [0.1, 1, 10, 100, 1000]:\n",
    "#    new_SVM = SVM('linear', c)\n",
    "#    new_ypred = new_SVM.predict(X_test)\n",
    "#    accuracy = accuracy_score(new_ypred, y_test)\n",
    "#    print('For c = ',c, ', accuracy = ',accuracy)\n",
    "\n",
    "#########################################    End\n",
    "\n",
    "class SVM(object):\n",
    "\n",
    "    def linear_kernel(self, x1, x2):                                                            # defining the kernel functions, using numpy vectorisation to speed up the process\n",
    "        #########################################    code to be filled a(ii)\n",
    "       \n",
    "        X_test = (X_test - mean_train)/std_train\n",
    "        \n",
    "        ###############################              End                                        \n",
    "\n",
    "    def __init__(self, kernel_str='linear', C=1.0, gamma=0.1):                                 # initializing the SVM class\n",
    "        if kernel_str == 'linear':\n",
    "            self.kernel = SVM.linear_kernel\n",
    "        else:\n",
    "            self.kernel = SVM.linear_kernel\n",
    "            print('Invalid kernel string, defaulting to linear.')\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.kernel_str = kernel_str\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        kernel_matrix = np.zeros((num_samples, num_samples))                                                    # creating the kernel matrix\n",
    "        kernel_matrix = self.kernel(self, X, X.T)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * kernel_matrix)                                                    # creating the matrices for the dual optimization problem, derivation explained in report\n",
    "        q = cvxopt.matrix(np.ones(num_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,num_samples)) * 1.\n",
    "        b = cvxopt.matrix(0) * 1.\n",
    "        G_upper = np.diag(np.ones(num_samples) * -1)\n",
    "        G_lower = np.identity(num_samples)\n",
    "        G = cvxopt.matrix(np.vstack((G_upper, G_lower)))\n",
    "        h_upper = np.zeros(num_samples)\n",
    "        h_lower = np.ones(num_samples) * self.C\n",
    "        h = cvxopt.matrix(np.hstack((h_upper, h_lower)))\n",
    "\n",
    "        solvers.options['show_progress'] = False                                                            # turning off the progress bar of cvxopt\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)                                                      # running the qp solver of cvxopt to solve the dual optimization problem\n",
    "        a = np.ravel(solution['x'])                                                                         # get the lagrange multipliers from the solution\n",
    "        support_vectors = a > 1e-4                                                                          # get the support vectors which have non-zero lagrange multipliers\n",
    "        ind = np.arange(len(a))[support_vectors]                                                            # get the indices of the support vectors for the kernel matrix\n",
    "        self.a = a[support_vectors]                                                                         # storing the data of the solution in the svm object\n",
    "        self.support_vectors = X[support_vectors]\n",
    "        self.y_support_vectors = y[support_vectors]\n",
    "        #print(\"%d support vectors out of %d points\" % (len(self.a), num_samples))\n",
    "\n",
    "        self.b = 0                                                                                          # deriving the bias value by enforcing the constraint for b in the svm optimization problem\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.y_support_vectors[n]\n",
    "            self.b -= np.sum(self.a * self.y_support_vectors * kernel_matrix[ind[n],support_vectors])\n",
    "        self.b /= len(self.a)\n",
    "\n",
    "        if self.kernel_str == 'linear':                                                                     # deriving the weights for the linear kernel\n",
    "            self.w = np.zeros(num_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.y_support_vectors[n] * self.support_vectors[n]\n",
    "        else:\n",
    "            self.w = None                                                                                   # if the kernel is not linear, then the weights are not defined\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.kernel_str == 'linear':                                                                     # if linear, then the prediction is given by the linear combination of the support vectors\n",
    "            #########################################    code to be filled a(iii)\n",
    "            for c in [0.1, 1, 10, 100, 1000]:\n",
    "                y_predict = fit(X_test, y_test)\n",
    "                accuracy = accuracy_score(y_predict, y_test)\n",
    "                print('For c = ',c, ', accuracy = ',accuracy)\n",
    "            ##############################              End\n",
    "        else:\n",
    "            y_predict = np.sum(self.a * self.y_support_vectors * self.kernel(self, X, self.support_vectors.T), axis=1)  # if not linear, then the prediction is given by the kernel modification to the standard linear version\n",
    "            #########################################    code to be filled a(iv)\n",
    "            for c in [0.1, 1, 10, 100, 1000]:\n",
    "                y_predict = fit(X_test, y_test)\n",
    "                accuracy = accuracy_score(y_predict, y_test)\n",
    "                print('For c = ',c, ', accuracy = ',accuracy)\n",
    "            ##############################              End\n",
    "\n",
    "# note that running on the full dataset is very slow (3-4 hours), so uncomment the code below and run this cell if you wish to check the results more quickly or apply grid search, comment it out again before running the full dataset\n",
    "X_train = X_train[:800]\n",
    "y_train = y_train[:800]\n",
    "X_test = X_test[:200]\n",
    "y_test = y_test[:200]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \"\"\"\n",
    "    ALERT: * * * No changes are allowed in this section  * * *\n",
    "    \"\"\"\n",
    "\n",
    "    input_data_one = sys.argv[1].strip()\n",
    "    \n",
    "    \"\"\"  Call to function that will perform the computation. \"\"\"\n",
    "    c_value = float(input_data_one)\n",
    "\n",
    "    svm_linear = SVM('linear', C=c_value)\n",
    "    svm_linear.fit(X_train, y_train)\n",
    "    y_pred_linear = svm_linear.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ad603-707a-4f1e-be20-c4db934710c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
